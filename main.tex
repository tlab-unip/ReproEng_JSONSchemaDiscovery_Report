
% VLDB template version of 2020-08-03 enhances the ACM template, version 1.7.0:
% https://www.acm.org/publications/proceedings-template
% The ACM Latex guide provides further information about the ACM template

\documentclass[sigconf, nonacm]{acmart}
\input{params.inc}


\begin{document}
\title{RepEng Project: A Replication Package for JSONSchemaDiscovery}

%%
%% The "author" command and its associated commands are used to define the authors and their affiliations.

\author{Qiang Tang}
\affiliation{%
	\institution{University of Passau}
	\city{Passau}
	\country{Germany}
}
\email{tang08@ads.uni-passau.de}

\maketitle

%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
\ifdefempty{\vldbavailabilityurl}{}{
	\vspace{.3cm}
	\begingroup\small\noindent\raggedright\textbf{Artifact Availability:}\\
	The source code, data, and/or other artifacts have been made available at \url{https://doi.org/10.5281/zenodo.10709946}.
	\endgroup
}
%%% VLDB block end %%%

\section{Introduction}

In 2018, the method known as JSONSchemaDiscovery\cite{jsonschema} was introduced, aimed at facilitating the extraction of schemas from JSON or extended JSON documents that are stored in a NoSQL document-oriented database or any other document repository. This method represents a significant advancement in the field of data management and schema extraction, catering to the evolving needs of handling unstructured or semi-structured data in modern database systems.

The research conducted has undeniably provided us a valuable tool for the extraction of JSON Schemas. However, it is important to acknowledge that, despite its utility, the initial structure and implementation of the original project posed certain challenges. These challenges were particularly evident for researchers and users who sought to replicate the research results. The complexities inherent in the original tool's design and operation made replication efforts cumbersome, highlighting a gap in accessibility and ease of use.

In response to these challenges, a replication package for JSONSchemaDiscovery has been developed. This package is designed to streamline the process of running the tool across various computing environments, thereby simplifying the execution of evaluation steps necessary for replicating the original research results.

The primary objective of this replication package is to validate the hypothesis that it is possible to replicate an experiment, as detailed in the original article, and achieve identical results. By doing so, the package aims to enhance the accessibility of JSONSchemaDiscovery, making it more user-friendly and facilitating broader adoption and application in research and practical scenarios.

\section{Package Overview}

\subsection{Challenges in Replication}

Replicating the research poses several challenges that need to be addressed meticulously. 

\subsubsection{Node Dependencies}

One of the primary challenges is the web application provided by the original research for facilitating extraction operations. This application, developed using node.js several years ago, now faces issues with many of its dependencies becoming deprecated. Attempting to build the application in its original form typically leads to errors, hindering the replication process. 

\subsubsection{Database Connection}

Another significant challenge is the accessibility and readiness of the datasets required for evaluation. These datasets are not only difficult to locate but some also require preprocessing to be usable for the intended purpose. Furthermore, the replication effort is complicated by the necessity of integrating the web application with a database. This integration is crucial for launching the application and for loading data into the database prior to performing any evaluation.

\subsection{Package Components}

To address these challenges, the replication package is equipped with a comprehensive set of components designed to facilitate a smoother replication process.

\subsubsection{Patches}

The package initially includes patches that are specifically designed to fix the versioning of node packages used by the web application, thereby mitigating build errors caused by deprecated dependencies.

\subsubsection{Docker Recipe}

A Docker recipe is provided, which is structured into separate stages. This structuring allows for independent building and caching of each stage. The build stage encompasses checking out the repository, applying necessary patches, and installing the project. The dependencies stage is responsible for pulling in remote dependencies, such as datasets and the report repository. During the deploy stage, dependencies required for running the experiment script and building the LaTeX report are installed. This stage also organizes the working directory in a manner that ensures useful results from previous stages are retained, while the final image is optimized to contain only the essential runtime dependencies, thereby reducing the image size.

\subsubsection{Docker Compose}

Docker Compose is employed to deploy both the application and MongoDB servers, thereby creating an environment that seamlessly connects the database with the client. This setup includes containers for both MongoDB and the application, with the former housing the data necessary for the JSON schema extraction tool and the test datasets, and the latter facilitating user operations within the app.

\subsubsection{Experiment Script}

The package also includes a doAll script, which is a comprehensive series of operations designed to ensure the evaluation process can be replicated reliably. This script begins with loading the datasets into the database. It then proceeds to assign extraction tasks to the web server through a series of requests. Following the extraction tasks, a schema along with related metadata is generated and incorporated into the report building process. This meticulous approach allows for the verification of the results against the expected outcomes, ensuring the reliability and accuracy of the replication process.

\section{Evaluation}

\subsection{Evaluation Criteria}

The original research diligently evaluated the tool's accuracy using various datasets, ensuring a robust assessment framework. A notable dataset employed for benchmarking purposes was the DBPedia dataset, as referenced in the work by Wang et al.\cite{schemamanagement}. These datasets, comprising multiple JSON files, each containing a list of JSON objects, provided a rich and comprehensive foundation for comparison and thorough evaluation.

The data extracted from the original article is presented in a table format as shown in \autoref{tab:results1}, offering a clear statistical overview of different datasets. Each row in the table provides specific statistical insights into individual datasets, while each column details the number of schemas extracted. This structured comparison method effectively highlights the performance of the JSON schema discovery tool against the methodology proposed by Wang et al., showcasing the tool's efficiency in a clear and measurable way.

\begin{table}[hb]% h asks to places the floating element [h]ere.
	\centering
	\caption{Comparison with Wang et al.}
	\label{tab:results1}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\multicolumn{2}{|c|}{Datasets} & \multicolumn{2}{c|}{JSON Schema Discovery} & Wang et al.                 \\
		\hline
		Collection                     & N\_JSON                                    & RS          & ROrd  & FS    \\
		\hline
		drugs                          & 3662                                       & 2818        & 2818  & 2818  \\
		\hline
		companies                      & 24367                                      & 21312       & 21312 & 21302 \\
		\hline
		movies                         & 30330                                      & 25140       & 25140 & 25137 \\
		\hline
	\end{tabular}
	\parbox{0.4\textwidth}{
		\raggedright\footnotesize
		N\_JSON - Number of JSON documents. RS - Raw schemas \newline
		ROrd - Raw schemas with ordered structure. FS - Final schemas.
	}
\end{table}

For our evaluation, we will employ the same DBPedia datasets to assess the replication package's ability to accurately reproduce results. The focus of this evaluation is to count the number of raw schemas extracted, taking into account both ordered and unordered structures.

Achieving a result that closely mirrors the table from the original research would confirm the replication package's effectiveness and the reliability of its performance in replicating the tool's functionality.

\subsection{Evaluation Results}

The replication package features a script designed for automating the execution of experiments and the generation of reports. To replicate the findings of the original research, one needs to run the doAll script within the container, following the initiation of the servers via Docker Compose. 

Conducted on a host machine equipped with an i5-9300H CPU and 32 GB memory, the experiment takes place within containers launched in WSL2. The execution of this script within the container leads to the experimental results being systematically displayed in the subsequent \autoref{tab:results2}. 

It is important to highlight that the results for each dataset were consistent, reinforcing the system's reliability and functional consistency. This consistency is crucial for validating the replication process, as it demonstrates the package's capability to reliably reproduce the original research outcomes, thereby affirming the tool's replicative accuracy and operational stability.

\begin{table}[hb]% h asks to places the floating element [h]ere.
	\centering
	\caption{Replication Results}
	\label{tab:results2}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\multicolumn{2}{|c|}{Datasets} & \multicolumn{2}{c|}{JSON Schema Discovery} \\
		\hline
		Collection                     & N\_JSON                                    & RS          & ROrd  \\
		\hline
		drugs                          & \drugsCount                                       & \drugsUnordered        & \drugsOrdered  \\
		\hline
		companies                      & \companiesCount                                      & \companiesUnordered       & \companiesOrdered \\
		\hline
		movies                         & \moviesCount                                      & \moviesUnordered       & \moviesOrdered  \\
		\hline
	\end{tabular}
	\parbox{0.4\textwidth}{
		\raggedright\footnotesize
		N\_JSON - Number of JSON documents. RS - Raw schemas \newline
		ROrd - Raw schemas with ordered structure. FS - Final schemas.
	}
\end{table}

\section{Conclusion}

Upon thorough evaluation of the package, it has been determined that the ability to replicate a specific result, as outlined in the original article, has been successfully achieved. This leads to the validation of the package's foundational hypothesis: it possesses the capability to accurately replicate an experiment detailed within the article, consistently producing the same outcomes.

It is important to note that the evaluation was conducted using a single dataset. Despite this limitation, the evidence gathered from the evaluation process sufficiently validates the package's replicative capabilities. Not reliant on any specfic setup, the package consistently produces identical results across various architectures, leading us to confidently assert its success as a replication tool.

%\clearpage

\bibliographystyle{ACM-Reference-Format}
\bibliography{main}

\end{document}
\endinput
